# idseq-workflows tests

## WDL task unit tests

`make test` from the top-level `Makefile` invokes [pytest](https://docs.pytest.org/en/stable/) on the WDL task unit tests in this folder. Each test case has an `inputs.json`, `expected_outputs.json`, copies of any files referenced therein, and a short `test_CASE_TASK.py` for pytest to discover. Several [pytest fixtures](https://docs.pytest.org/en/stable/fixture.html) supply (override-able) boilerplate logic for each case to:

1. Load the WDL task from source code in this repo
2. Load the inputs and expected outputs from the case folder
3. Run miniwdl to get the actual task outputs given the test inputs
4. Structured comparison of actual and expected outputs

The fixture implementations can be viewed in `conftest.py` which pytest discovers (one at the top level; and a short one in each task folder which loads the corresponding WDL source & makes it available to test cases in its subfolders). The [root `conftest.py`](conftest.py) also contains an `INPUT_OVERRIDES` dict supplying some inputs common to many/all tasks (e.g. docker image tag).

The initial skeleton of task test cases has been generated by running the full workflow on a [synthetic benchmarking dataset](https://github.com/chanzuckerberg/idseq-bench) ("bench3") and recording all the intermediate inputs and outputs. The tests require actual and expected non-file outputs to be identical, and file outputs to have the same basenames and sizes. These are meant to be refined and added to, of course.

Start by looking through the [short-read-mngs/host_filter/tasks/RunValidateInput/bench3](short-read-mngs/host_filter/tasks/RunValidateInput/bench3) and [short-read-mngs/host_filter/tasks/RunValidateInput/invalid_fastq](short-read-mngs/host_filter/tasks/RunValidateInput/invalid_fastq) cases, which illustrate a positive and expected error case, respectively. New test cases for a task can be created either by adding test functions to an existing `test_*.py` (if it can reuse the folder's input and expected output files), or by populating new folders alongside the existing ones with their own `inputs.json`, `expected_outputs.json`, referenced files, and `test_*.py`.
